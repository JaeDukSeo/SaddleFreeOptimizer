{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Test for Saddle-Free Optimizer\n",
    "\n",
    "> Copyright 2019 Dave Fernandes. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    ">\n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    ">  \n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This example trains an autoencoder on MNIST data using either the ADAM optimizer or the Saddle-Free (SF) method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from SFOptimizer import SFOptimizer\n",
    "from SFOptimizer import SFDamping\n",
    "from mnist import dataset\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Create a layer with sigmoid activation. Weights have a sparse random initialization as per [Martens \\(2010\\)](http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = []\n",
    "\n",
    "def logistic_layer(layer_name, input_layer, hidden_units, n_random):\n",
    "    initial_W = np.zeros((input_layer.shape[1], hidden_units))\n",
    "    for i in range(hidden_units):\n",
    "        column = np.zeros((input_layer.shape[1], 1))\n",
    "        column[0:n_random,:] += np.random.randn(n_random, 1)\n",
    "        np.random.shuffle(column)\n",
    "        initial_W[:, i:i+1] = column\n",
    "    \n",
    "    with tf.name_scope('layer_' + layer_name):\n",
    "        W = tf.get_variable('W_' + layer_name, initializer=tf.convert_to_tensor(initial_W, dtype=tf.float64), use_resource=True)\n",
    "        b = tf.get_variable('b_' + layer_name, [hidden_units], initializer=tf.zeros_initializer(), dtype=tf.float64, use_resource=True)\n",
    "        y = tf.sigmoid(tf.matmul(input_layer, W) + b)\n",
    "    \n",
    "    var_list.append(W)\n",
    "    var_list.append(b)\n",
    "    return W, b, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep autoencoder network from [Hinton & Salakhutdinov \\(2006\\)](https://www.cs.toronto.edu/~hinton/science.pdf). This example is used as a standard test in several optimization papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AE_model():\n",
    "    n_inputs = 28*28\n",
    "    n_hidden1 = 1000\n",
    "    n_hidden2 = 500\n",
    "    n_hidden3 = 250\n",
    "    n_hidden4 = 30\n",
    "    \n",
    "    x = tf.placeholder(tf.float64, shape=(None, n_inputs), name='input')\n",
    "\n",
    "    with tf.name_scope('dnn'):\n",
    "        _, _, y1 = logistic_layer('1', x, n_hidden1, 15)\n",
    "        _, _, y2 = logistic_layer('2', y1, n_hidden2, 15)\n",
    "        _, _, y3 = logistic_layer('3', y2, n_hidden3, 15)\n",
    "        \n",
    "        W4, b4, _ = logistic_layer('4', y3, n_hidden4, 15)\n",
    "        y4 = tf.matmul(y3, W4) + b4\n",
    "        \n",
    "        _, _, y5 = logistic_layer('5', y4, n_hidden3, 15)\n",
    "        _, _, y6 = logistic_layer('6', y5, n_hidden2, 15)\n",
    "        _, _, y7 = logistic_layer('7', y6, n_hidden1, 15)\n",
    "        W8, b8, y_out = logistic_layer('8', y7, n_inputs, 15)\n",
    "        y_logits = tf.matmul(y7, W8) + b8\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=y_logits)\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        error = tf.reduce_mean(tf.reduce_sum(tf.squared_difference(x, y_out), axis=1))\n",
    "\n",
    "    return x, loss, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "Saves weights to data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = os.path.join(os.getcwd(), 'data', 'ae_weights')\n",
    "\n",
    "def MNIST_AE_test(use_SF, start_from_previous_run):\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(\"./data/\")\n",
    "\n",
    "    x, loss, error = AE_model()\n",
    "    saver = tf.train.Saver(var_list)\n",
    "\n",
    "    if use_SF:\n",
    "        max_epochs = 10\n",
    "        batch_size = 2000\n",
    "        print_interval = 1\n",
    "    else:\n",
    "        max_epochs = 1000\n",
    "        batch_size = 200\n",
    "        print_interval = 100\n",
    "    \n",
    "    if use_SF:\n",
    "        # See SFOptimizer.py for options\n",
    "        optimizer = SFOptimizer(var_list, krylov_dimension=100, damping_type=SFDamping.marquardt, dtype=tf.float64)\n",
    "    else:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "\n",
    "    print('Initializing...')\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if start_from_previous_run:\n",
    "        saver.restore(sess, model_filepath)\n",
    "    \n",
    "    print('Constructing graph...')\n",
    "    if use_SF:\n",
    "        big_train_op = optimizer.minimize(loss)\n",
    "        little_train_op = optimizer.fixed_subspace_step()\n",
    "        update_op = optimizer.update()\n",
    "        reset_op = optimizer.reset_lambda()\n",
    "\n",
    "    history = []\n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for epoch in range(max_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        total_error = 0.0\n",
    "        \n",
    "        for iteration in range(n_batches):\n",
    "            if iteration % print_interval == 0:\n",
    "                print('-- Epoch:', epoch + 1, ' Batch:', iteration + 1, '/', n_batches, '--')\n",
    "\n",
    "            x_batch, t_batch = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x: x_batch}\n",
    "            \n",
    "            if use_SF:\n",
    "                # Reset the damping parameter\n",
    "                _ = sess.run(reset_op)\n",
    "                \n",
    "                # Compute Krylov subspace and take one training step\n",
    "                initial_loss, initial_lambda, _ = sess.run([loss, optimizer.lambda_damp, big_train_op], feed_dict=feed_dict)\n",
    "                final_loss, rho, _ = sess.run([loss, optimizer.rho, update_op], feed_dict=feed_dict)\n",
    "                \n",
    "                if iteration % print_interval == 0:\n",
    "                    print('    Loss_i:', initial_loss, 'Loss_f:', final_loss, 'rho', rho, 'lambda:', initial_lambda)\n",
    "                \n",
    "                # Take up to 5 more steps without recomputing the Krylov subspace\n",
    "                for little_step in range(5):\n",
    "                    initial_loss, initial_lambda, _ = sess.run([loss, optimizer.lambda_damp, little_train_op], feed_dict=feed_dict)\n",
    "                    final_loss, rho, _ = sess.run([loss, optimizer.rho, update_op], feed_dict=feed_dict)\n",
    "                    \n",
    "                    if iteration % print_interval == 0:\n",
    "                        print('    Loss_i:', initial_loss, 'Loss_f:', final_loss, 'rho', rho, 'lambda:', initial_lambda)\n",
    "                \n",
    "                error_train = sess.run(error, feed_dict=feed_dict)\n",
    "            else:\n",
    "                # Take a gradient descent step\n",
    "                sess.run(train_op, feed_dict=feed_dict)\n",
    "                initial_loss, error_train = sess.run([loss, error], feed_dict=feed_dict)\n",
    "                \n",
    "                if iteration % print_interval == 0:\n",
    "                    print('    Loss:', initial_loss)\n",
    "            \n",
    "            history += [error_train]\n",
    "            total_error += error_train\n",
    "            \n",
    "            if iteration % print_interval == 0:\n",
    "                print('    Train error:', error_train)\n",
    "\n",
    "        error_train = total_error / n_batches\n",
    "        error_test = sess.run(error, feed_dict={x: mnist.test.images})\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "\n",
    "        print('\\n*** Epoch:', epoch + 1, 'Train error:', error_train, ' Test error:', error_test, ' Time:', dt, 'sec\\n')\n",
    "        save_path = saver.save(sess, model_filepath)\n",
    "    \n",
    "    sess.close()\n",
    "    return history, optimizer.get_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train with `use_SF = False` to use the ADAM method, and with `use_SF = True` to use the Saddle-Free method.\n",
    "* Train with `start_from_previous_run = False` to start from random initialization, and with `start_from_previous_run = True` to start from where you previously left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history, opt_name = MNIST_AE_test(use_SF = True, start_from_previous_run = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the error versus training step. For reference, the previous best error obtained by a first order method was 1.0 \\([Sutskever, _et al_., 2013](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\\), and by the SF method was 0.57 \\([Dauphin, _et al_., 2014](https://arxiv.org/abs/1406.2572)\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "plt.plot(history)\n",
    "plt.ylabel('MSE')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Steps')\n",
    "#plt.xscale('log')\n",
    "plt.title(opt_name + ' Optimizer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
