{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Test for Saddle-Free Optimizer\n",
    "\n",
    "Copyright 2018 Dave Fernandes. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "  http://www.apache.org/licenses/LICENSE-2.0\n",
    "  \n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This example trains an autoencoder on MNIST data using either SGD with momentum or the Saddle-Free (SF) method. You can train using SGD first for 2000 epochs until it plateaus, and then use the SF method to further train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from SFOptimizer import SFOptimizer\n",
    "from SFOptimizer import SFDamping\n",
    "from mnist import dataset\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = []\n",
    "\n",
    "def logistic_layer(layer_name, input_layer, hidden_units, n_random):\n",
    "    # Initialize weights with sparse random values as per Martens (2010)\n",
    "    initial_W = np.zeros((input_layer.shape[1], hidden_units))\n",
    "    for i in range(hidden_units):\n",
    "        column = np.zeros((input_layer.shape[1], 1))\n",
    "        column[0:n_random,:] += np.random.randn(n_random, 1)\n",
    "        np.random.shuffle(column)\n",
    "        initial_W[:, i:i+1] = column\n",
    "    \n",
    "    with tf.name_scope('layer_' + layer_name):\n",
    "        W = tf.get_variable('W_' + layer_name, initializer=tf.convert_to_tensor(initial_W, dtype=tf.float64), use_resource=True)\n",
    "        b = tf.get_variable('b_' + layer_name, [hidden_units], initializer=tf.zeros_initializer(), dtype=tf.float64, use_resource=True)\n",
    "        y = tf.sigmoid(tf.matmul(input_layer, W) + b)\n",
    "    \n",
    "    var_list.append(W)\n",
    "    var_list.append(b)\n",
    "    return W, b, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Deep autoencoder network from Hinton & Salakhutdinov (2006) \"\"\"\n",
    "def AE_model():\n",
    "    n_inputs = 28*28\n",
    "    n_hidden1 = 1000\n",
    "    n_hidden2 = 500\n",
    "    n_hidden3 = 250\n",
    "    n_hidden4 = 30\n",
    "    \n",
    "    x = tf.placeholder(tf.float64, shape=(None, n_inputs), name='input')\n",
    "\n",
    "    with tf.name_scope('dnn'):\n",
    "        _, _, y1 = logistic_layer('1', x, n_hidden1, 15)\n",
    "        _, _, y2 = logistic_layer('2', y1, n_hidden2, 15)\n",
    "        _, _, y3 = logistic_layer('3', y2, n_hidden3, 15)\n",
    "        \n",
    "        W4, b4, _ = logistic_layer('4', y3, n_hidden4, 15)\n",
    "        y4 = tf.matmul(y3, W4) + b4\n",
    "        \n",
    "        _, _, y5 = logistic_layer('5', y4, n_hidden3, 15)\n",
    "        _, _, y6 = logistic_layer('6', y5, n_hidden2, 15)\n",
    "        _, _, y7 = logistic_layer('7', y6, n_hidden1, 15)\n",
    "        W8, b8, y_out = logistic_layer('8', y7, n_inputs, 15)\n",
    "        y_logits = tf.matmul(y7, W8) + b8\n",
    "\n",
    "    saver = tf.train.Saver(var_list)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=y_logits)\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        error = tf.reduce_mean(tf.reduce_sum(tf.squared_difference(x, y_out), axis=1))\n",
    "\n",
    "    return x, loss, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = os.path.join(os.getcwd(), 'data', 'ae_weights')\n",
    "\n",
    "def MNIST_AE_test(use_SF, start_from_previous_run):\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(\"./data/\")\n",
    "\n",
    "    x, loss, error = AE_model()\n",
    "    saver = tf.train.Saver(var_list)\n",
    "\n",
    "    if use_SF:\n",
    "        max_epochs = 5\n",
    "        batch_size = 2000\n",
    "        print_interval = 1\n",
    "    else:\n",
    "        max_epochs = 2000\n",
    "        batch_size = 200\n",
    "        print_interval = 100\n",
    "    \n",
    "    if use_SF:\n",
    "        # See SFOptimizer.py for options\n",
    "        optimizer = SFOptimizer(var_list, krylov_dimension=100, damping_type=SFDamping.marquardt, dtype=tf.float64)\n",
    "    else:\n",
    "        # See Sutskever et al. (2013)\n",
    "        #optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.99)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "\n",
    "    print('Initializing...')\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if start_from_previous_run:\n",
    "        saver.restore(sess, model_filepath)\n",
    "    \n",
    "    print('Constructing graph...')\n",
    "    if use_SF:\n",
    "        big_train_op = optimizer.minimize(loss)\n",
    "        little_train_op = optimizer.fixed_subspace_step()\n",
    "        update_op = optimizer.update()\n",
    "        reset_op = optimizer.reset_lambda()\n",
    "\n",
    "    history = []\n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for epoch in range(max_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        \n",
    "        for iteration in range(n_batches):\n",
    "            if iteration % print_interval == 0:\n",
    "                print('-- Epoch:', epoch + 1, ' Batch:', iteration + 1, '/', n_batches, '--')\n",
    "\n",
    "            x_batch, t_batch = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x: x_batch}\n",
    "            \n",
    "            if use_SF:\n",
    "                # Reset the damping parameter\n",
    "                _ = sess.run(reset_op)\n",
    "                \n",
    "                # Compute Krylov subspace and take one training step\n",
    "                initial_loss, initial_lambda, _ = sess.run([loss, optimizer.lambda_damp, big_train_op], feed_dict=feed_dict)\n",
    "                final_loss, rho, _ = sess.run([loss, optimizer.rho, update_op], feed_dict=feed_dict)\n",
    "                \n",
    "                if iteration % print_interval == 0:\n",
    "                    print('    Loss_i:', initial_loss, 'Loss_f:', final_loss, 'rho', rho, 'lambda:', initial_lambda)\n",
    "                \n",
    "                # Take up to 5 more steps without recomputing the Krylov subspace\n",
    "                for little_step in range(5):\n",
    "                    initial_loss, initial_lambda, _ = sess.run([loss, optimizer.lambda_damp, little_train_op], feed_dict=feed_dict)\n",
    "                    final_loss, rho, _ = sess.run([loss, optimizer.rho, update_op], feed_dict=feed_dict)\n",
    "                    \n",
    "                    if iteration % print_interval == 0:\n",
    "                        print('    Loss_i:', initial_loss, 'Loss_f:', final_loss, 'rho', rho, 'lambda:', initial_lambda)\n",
    "            else:\n",
    "                # Take a gradient descent step\n",
    "                i = iteration + (epoch * n_batches)\n",
    "                \n",
    "                sess.run(train_op, feed_dict=feed_dict)\n",
    "                initial_loss = sess.run(loss, feed_dict=feed_dict)\n",
    "                \n",
    "                if iteration % print_interval == 0:\n",
    "                    print('    Loss:', initial_loss)\n",
    "            \n",
    "            history += [initial_loss]\n",
    "            \n",
    "            if iteration % print_interval == 0:\n",
    "                error_train = sess.run(error, feed_dict=feed_dict)\n",
    "                print('    Train error:', error_train)\n",
    "\n",
    "        error_train = sess.run(error, feed_dict=feed_dict)\n",
    "        error_test = sess.run(error, feed_dict={x: mnist.test.images})\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "\n",
    "        print('\\n*** Epoch:', epoch + 1, 'Train error:', error_train, ' Test error:', error_test, ' Time:', dt, 'sec\\n')\n",
    "        save_path = saver.save(sess, model_filepath)\n",
    "    \n",
    "    return history, optimizer.get_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First train with `use_SF = False` and `start_from_previous_run = False`.\n",
    "Then train with `use_SF = True` and `start_from_previous_run = True` to use the SF method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-56f46c9e04de>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/davef/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/davef/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/davef/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/davef/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Initializing...\n",
      "Constructing graph...\n",
      "Training...\n",
      "-- Epoch: 1  Batch: 1 / 275 --\n",
      "    Loss: 0.9177406902456087\n",
      "    Train error: 216.58256535214582\n",
      "-- Epoch: 1  Batch: 101 / 275 --\n",
      "    Loss: 0.21997505451413732\n",
      "    Train error: 40.219384555975346\n",
      "-- Epoch: 1  Batch: 201 / 275 --\n",
      "    Loss: 0.16507290147817644\n",
      "    Train error: 26.082189695190873\n",
      "\n",
      "*** Epoch: 1 Train error: 20.776535906337585  Test error: 20.726957003474443  Time: 46.57620415103156 sec\n",
      "\n",
      "-- Epoch: 2  Batch: 1 / 275 --\n",
      "    Loss: 0.14567630646619117\n",
      "    Train error: 21.38029581392212\n",
      "-- Epoch: 2  Batch: 101 / 275 --\n",
      "    Loss: 0.1313457058378491\n",
      "    Train error: 17.48794239295931\n",
      "-- Epoch: 2  Batch: 201 / 275 --\n",
      "    Loss: 0.12670295846190682\n",
      "    Train error: 16.258324921140655\n",
      "\n",
      "*** Epoch: 2 Train error: 14.963811059233375  Test error: 14.066543460049742  Time: 52.019009247014765 sec\n",
      "\n",
      "-- Epoch: 3  Batch: 1 / 275 --\n",
      "    Loss: 0.117177814174857\n",
      "    Train error: 13.888680428518507\n",
      "-- Epoch: 3  Batch: 101 / 275 --\n",
      "    Loss: 0.11743300243678616\n",
      "    Train error: 13.605434802041337\n",
      "-- Epoch: 3  Batch: 201 / 275 --\n",
      "    Loss: 0.10794498724454728\n",
      "    Train error: 11.564075395886475\n",
      "\n",
      "*** Epoch: 3 Train error: 12.187188687844625  Test error: 11.571174490188387  Time: 50.439798237988725 sec\n",
      "\n",
      "-- Epoch: 4  Batch: 1 / 275 --\n",
      "    Loss: 0.108450965672382\n",
      "    Train error: 11.421902323258255\n",
      "-- Epoch: 4  Batch: 101 / 275 --\n",
      "    Loss: 0.10411557289602526\n",
      "    Train error: 10.859483270392566\n",
      "-- Epoch: 4  Batch: 201 / 275 --\n",
      "    Loss: 0.10691894186690984\n",
      "    Train error: 10.7441012415998\n",
      "\n",
      "*** Epoch: 4 Train error: 9.999076120305988  Test error: 9.912969104375888  Time: 49.24528424500022 sec\n",
      "\n",
      "-- Epoch: 5  Batch: 1 / 275 --\n",
      "    Loss: 0.09899998383229819\n",
      "    Train error: 9.59276193496662\n",
      "-- Epoch: 5  Batch: 101 / 275 --\n",
      "    Loss: 0.1002168509696023\n",
      "    Train error: 9.824793304900442\n",
      "-- Epoch: 5  Batch: 201 / 275 --\n",
      "    Loss: 0.09410666927323533\n",
      "    Train error: 8.72848133690602\n",
      "\n",
      "*** Epoch: 5 Train error: 9.147921232565764  Test error: 8.736257531435308  Time: 49.71104683598969 sec\n",
      "\n",
      "-- Epoch: 6  Batch: 1 / 275 --\n",
      "    Loss: 0.09735549909407767\n",
      "    Train error: 8.681598942682736\n",
      "-- Epoch: 6  Batch: 101 / 275 --\n",
      "    Loss: 0.09740483727543013\n",
      "    Train error: 8.510554842754711\n",
      "-- Epoch: 6  Batch: 201 / 275 --\n",
      "    Loss: 0.09344331022116732\n",
      "    Train error: 8.003313608596288\n",
      "\n",
      "*** Epoch: 6 Train error: 7.922321020643765  Test error: 7.9012131564657055  Time: 52.8812286320026 sec\n",
      "\n",
      "-- Epoch: 7  Batch: 1 / 275 --\n",
      "    Loss: 0.09681353257309165\n",
      "    Train error: 8.186191793176357\n",
      "-- Epoch: 7  Batch: 101 / 275 --\n",
      "    Loss: 0.09455321249485657\n",
      "    Train error: 8.065926084783994\n",
      "-- Epoch: 7  Batch: 201 / 275 --\n",
      "    Loss: 0.09143369234200292\n",
      "    Train error: 7.586357012136711\n",
      "\n",
      "*** Epoch: 7 Train error: 7.607625580468332  Test error: 7.282246789412585  Time: 49.669477357005235 sec\n",
      "\n",
      "-- Epoch: 8  Batch: 1 / 275 --\n",
      "    Loss: 0.08683400467742343\n",
      "    Train error: 6.888092021751115\n",
      "-- Epoch: 8  Batch: 101 / 275 --\n",
      "    Loss: 0.09027050559678444\n",
      "    Train error: 7.233935808240674\n",
      "-- Epoch: 8  Batch: 201 / 275 --\n",
      "    Loss: 0.08497602821347396\n",
      "    Train error: 6.409388601806281\n",
      "\n",
      "*** Epoch: 8 Train error: 7.507535314542406  Test error: 7.1886064360329645  Time: 49.9953629449592 sec\n",
      "\n",
      "-- Epoch: 9  Batch: 1 / 275 --\n",
      "    Loss: 0.08809513308112349\n",
      "    Train error: 6.686395746381096\n",
      "-- Epoch: 9  Batch: 101 / 275 --\n",
      "    Loss: 0.09030269141743594\n",
      "    Train error: 6.982922824656771\n",
      "-- Epoch: 9  Batch: 201 / 275 --\n",
      "    Loss: 0.0869651286420556\n",
      "    Train error: 6.474210194132484\n",
      "\n",
      "*** Epoch: 9 Train error: 6.544237540008953  Test error: 6.594568975797901  Time: 49.97094486199785 sec\n",
      "\n",
      "-- Epoch: 10  Batch: 1 / 275 --\n",
      "    Loss: 0.09185212409266154\n",
      "    Train error: 6.973035999543802\n",
      "-- Epoch: 10  Batch: 101 / 275 --\n",
      "    Loss: 0.08683318960716248\n",
      "    Train error: 6.5820756232592315\n",
      "-- Epoch: 10  Batch: 201 / 275 --\n",
      "    Loss: 0.08890848147754811\n",
      "    Train error: 6.856548527151906\n",
      "\n",
      "*** Epoch: 10 Train error: 6.724411931304494  Test error: 6.482108245173861  Time: 50.391991213022266 sec\n",
      "\n",
      "-- Epoch: 11  Batch: 1 / 275 --\n",
      "    Loss: 0.08656109679123741\n",
      "    Train error: 6.489539351230005\n",
      "-- Epoch: 11  Batch: 101 / 275 --\n",
      "    Loss: 0.08421910114541235\n",
      "    Train error: 6.197466059512464\n",
      "-- Epoch: 11  Batch: 201 / 275 --\n",
      "    Loss: 0.08449261314358307\n",
      "    Train error: 6.204504165986655\n",
      "\n",
      "*** Epoch: 11 Train error: 6.445962138833436  Test error: 6.073052260526548  Time: 49.09489113203017 sec\n",
      "\n",
      "-- Epoch: 12  Batch: 1 / 275 --\n",
      "    Loss: 0.08361631562941198\n",
      "    Train error: 5.693999330114769\n",
      "-- Epoch: 12  Batch: 101 / 275 --\n",
      "    Loss: 0.08646146744010431\n",
      "    Train error: 6.288249762215144\n",
      "-- Epoch: 12  Batch: 201 / 275 --\n",
      "    Loss: 0.08297601694598195\n",
      "    Train error: 5.651769819423637\n",
      "\n",
      "*** Epoch: 12 Train error: 5.779317249722217  Test error: 5.780968009197168  Time: 52.470744789985474 sec\n",
      "\n",
      "-- Epoch: 13  Batch: 1 / 275 --\n",
      "    Loss: 0.08239781042618964\n",
      "    Train error: 5.3345170096045536\n",
      "-- Epoch: 13  Batch: 101 / 275 --\n",
      "    Loss: 0.08764419993600665\n",
      "    Train error: 5.836087694651308\n",
      "-- Epoch: 13  Batch: 201 / 275 --\n",
      "    Loss: 0.08536294612065598\n",
      "    Train error: 5.718899486658321\n",
      "\n",
      "*** Epoch: 13 Train error: 5.804010334505664  Test error: 5.652727707790161  Time: 50.73629179300042 sec\n",
      "\n",
      "-- Epoch: 14  Batch: 1 / 275 --\n",
      "    Loss: 0.08221175717400665\n",
      "    Train error: 5.497725130454235\n",
      "-- Epoch: 14  Batch: 101 / 275 --\n",
      "    Loss: 0.08445086665221958\n",
      "    Train error: 5.655770031400383\n",
      "-- Epoch: 14  Batch: 201 / 275 --\n",
      "    Loss: 0.08225901832179976\n",
      "    Train error: 5.445785435917\n",
      "\n",
      "*** Epoch: 14 Train error: 5.579741304707881  Test error: 5.486713007481607  Time: 49.927194982999936 sec\n",
      "\n",
      "-- Epoch: 15  Batch: 1 / 275 --\n",
      "    Loss: 0.08349816977424425\n",
      "    Train error: 5.592098213125325\n"
     ]
    }
   ],
   "source": [
    "history, opt_name = MNIST_AE_test(use_SF = False, start_from_previous_run = False)\n",
    "    \n",
    "# Plot the cost\n",
    "plt.plot(history)\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Steps')\n",
    "plt.title(opt_name + ' Optimizer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
